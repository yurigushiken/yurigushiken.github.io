---
layout: post
title: "Columbia AI Summit poster contribution"
date: 2025-03-04 14:00:00 -0400
categories: academic research lcn-lab
thumbnail: https://raw.githubusercontent.com/yurigushiken/yurigushiken.github.io/main/media/poster-eye-track-ml.jpg
subtitle: "Poster presentation at Columbia AI Summit."
---

<div style="padding: 15px; border: 1px solid #555; border-radius: 5px; margin-bottom: 20px; background-color: #333a45;">
  <h3 style="margin-top: 0; color: #eee;">What it is:</h3>
  <p style="font-size: 1.1em; color: #ccc;">Poster presentation at Columbia AI Summit. <a href="https://yurigushiken.github.io/academic/research/lcn-lab/2025/03/04/Columbia-AI-Summit-poster-contribution.html" style="color: #7cc5ff;">Eye-Track-ML: our machine learning pipeline designed to automate the frame-by-frame coding of extensive eye-tracking video data.</a></p>
  
  <h3 style="color: #eee;">What we did:</h3>
  <ul style="font-size: 1.1em; list-style-type: disc; padding-left: 20px; color: #ccc;">
    <li>Develop an automated pipeline to analyze extensive eye-tracking video data from infant studies.</li>
    <li>Employ computer vision models (YOLOv11 and SAM2.1) for precise object detection, segmentation, and event classification.</li>
    <li>Attain 100% accuracy for event labeling and approximately 94% for object labeling.</li>
    <li>Decrease manual coding labor by over 90%, ensuring high data consistency.</li>
  </ul>
</div>

<link rel="stylesheet" href="/assets/css/carousel.css">

<div class="image-carousel">
  <div class="carousel-slides">
    <div class="carousel-slide">
      <img src="https://raw.githubusercontent.com/yurigushiken/yurigushiken.github.io/main/media/poster-eye-track-ml.jpg"
           alt="Eye-Track-ML Poster" />
      <p><em>Eye-Track-ML Poster - <a href="https://drive.google.com/file/d/1lyNk8Ul3Oee55g5OEn9xrU0beRvYMDWP/view" target="_blank">Download PDF</a></em></p>
    </div>
    <div class="carousel-slide">
      <img src="https://raw.githubusercontent.com/yurigushiken/yurigushiken.github.io/main/media/columbiaAIsummeit-20250304_133837.jpg"
           alt="Columbia AI Summit 3" />
       <p><em>Presenting at the Columbia AI Summit</em></p>
    </div>
    <div class="carousel-slide">
      <img src="https://raw.githubusercontent.com/yurigushiken/yurigushiken.github.io/main/media/columbiaAIsummeit-20250304_123812.jpg"
           alt="Columbia AI Summit 2" />
       <p><em>Discussing the poster</em></p>
    </div>
    <div class="carousel-slide">
      <img src="https://raw.githubusercontent.com/yurigushiken/yurigushiken.github.io/main/media/columbiaAIsummeit-20250304_144451.jpg"
           alt="Columbia AI Summit 1" />
        <p><em>AI Summit venue</em></p>
    </div>
  </div>
  <button class="carousel-button prev">&#10094;</button>
  <button class="carousel-button next">&#10095;</button>
  <div class="carousel-thumbnails">
    <!-- Thumbnails will be generated by JS -->
  </div>
</div>

<script src="/assets/js/carousel.js"></script>

<!-- Trivial change to force git update -->
<details style="margin-bottom: 20px; background-color: #282c34; padding: 15px; border-radius: 5px; border: 1px solid #444;">
  <summary style="cursor: pointer; font-weight: bold; color: #7cc5ff; font-size: 1.2em;">Read Full Project Details...</summary>
  <div style="padding-top: 15px; color: #bbb;" markdown="1">

## Eye-Track-ML: A Machine Learning Pipeline for Automated Frame-by-Frame Coding of Eye-Tracking Videos

Our project, Eye-Track-ML, is a pipeline for automating eye-tracking video analysis using computer vision models, YOLOv11 and SAM2.1. We developed this solution to address the challenge of manually coding over 6+ hours of video data (600,000 frames) from our infant event representation study. The pipeline combines YOLOv11 for image classification and object detection with SAM2.1 for object segmentation. 
For event labeling our pipeline achieves 100% accuracy, and for object labeling we achieve ~94%.

We found that human verification remains necessary for detecting subtle patterns and edge cases. However, our system establishes a strong baseline for consistency, requiring human verifiers to correct only about ~6% of data points. This is a dramatic reduction in manual labor. 

[YOLO documentation](https://docs.ultralytics.com/)
[Segment Anything Model (SAM) repository](https://github.com/facebookresearch/segment-anything)
[How to train YOLOv11 on custom data](https://blog.roboflow.com/yolov11-how-to-train-custom-data/)
[Fine-tuning SAM 2.1](https://blog.roboflow.com/fine-tune-sam-2-1/)

  </div>
</details>
 